{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import Attention\n",
    "from loss import NLLLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from generalRnn import BaseCoder\n",
    "\n",
    "class Encoder(BaseCoder):\n",
    "    def __init__(self,vocab_size, hidden_size, embedding_size, input_dropout=0.0,output_dropout=0.0, n_layers=1, bidirectional=True,rnn=\"lstm\"):\n",
    "        super(Encoder, self).__init__(vocab_size, hidden_size, embedding_size, input_dropout,output_dropout, n_layers, rnn)\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "\n",
    "        # TODO: add pretrained embeddings\n",
    "\n",
    "        self.rnn = self.baseModel(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers,\n",
    "                    batch_first=True, bidirectional=bidirectional, dropout=output_dropout)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # embedded = self.input_dropout(embedded)\n",
    "        # embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from generalRnn import BaseCoder\n",
    "from attention import Attention\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Decoder(BaseCoder):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_size, input_dropout=0.0, output_dropout=0.0, n_layers=1, bidirectional=False,rnn=\"lstm\"):\n",
    "        super(Decoder,self).__init__(vocab_size, hidden_size,embedding_size,input_dropout,output_dropout, n_layers, rnn)\n",
    "        self.rnn = self.baseModel(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, \n",
    "                    batch_first=True,dropout=output_dropout)\n",
    "        self.output_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        # temporary set attention embedding size to hidden size\n",
    "        self.attention = Attention(self.hidden_size)\n",
    "\n",
    "        self.wsm = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_seq, encoder_hidden, encoder_outputs, func=F.log_softmax):\n",
    "        # batch_size = input_seq.size(0)\n",
    "        max_length = input_seq.size(1)\n",
    "\n",
    "        # using cuda or not\n",
    "        inputs = input_seq\n",
    "        \n",
    "        # for bidrectional encoder\n",
    "        # encoder_hidden: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        decoder_hidden = tuple([torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2) for h in encoder_hidden])\n",
    "\n",
    "        outputs = []\n",
    "        # lengths = np.array([max_length] * batch_size)\n",
    "\n",
    "        prev = inputs[:, 0].unsqueeze(1)\n",
    "        for i in range(max_length):\n",
    "            softmax, decoder_hidden, attention = self.forward_helper(prev, decoder_hidden,encoder_outputs ,func)\n",
    "            output_seq = softmax.squeeze(1) # batch * seq_length\n",
    "            outputs.append(output_seq)\n",
    "            prev = output_seq.topk(1)[1] # max probability index\n",
    "\n",
    "        return outputs,decoder_hidden\n",
    "            \n",
    "\n",
    "\n",
    "    # could insert one parameter like: src_matrix\n",
    "    def forward_helper(self, decoder_input, decoder_hidden, encoder_outputs, func):\n",
    "        batch_size = decoder_input.size(0)\n",
    "        output_size = decoder_input.size(1)\n",
    "        embedded = self.embedding(decoder_input)\n",
    "        # embedded = self.input_dropout(embedded)\n",
    "        output,hidden = self.rnn(embedded, decoder_hidden)\n",
    "        output, attention = self.attention(output, encoder_outputs) # attention\n",
    "        softmax = func(self.wsm(output.view(-1, self.hidden_size)), dim=1).view(batch_size,output_size,-1)\n",
    "        return softmax, hidden, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 6, 7, 1, 7, 6, 4, 0, 1, 1],\n",
       "        [1, 1, 0, 6, 5, 7, 8, 5, 8, 6],\n",
       "        [9, 6, 8, 7, 9, 6, 8, 5, 8, 4],\n",
       "        [6, 9, 1, 4, 2, 4, 6, 0, 6, 0],\n",
       "        [4, 5, 7, 9, 0, 2, 9, 1, 6, 2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sents = torch.LongTensor(5,10).random_(0,10) # batch * seqlen\n",
    "src_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 1, 9, 0, 3, 0, 6, 8, 1, 4],\n",
       "        [8, 9, 7, 2, 1, 2, 4, 5, 6, 2],\n",
       "        [7, 9, 7, 0, 6, 1, 7, 5, 0, 9],\n",
       "        [7, 2, 1, 8, 4, 1, 2, 5, 6, 6],\n",
       "        [8, 6, 4, 8, 0, 4, 8, 9, 6, 4]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_sents = torch.LongTensor(5,10).random_(0,10)\n",
    "tgt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(10,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder.forward(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(10,2*10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nmt/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "result = decoder.forward(output_var,encoder_hidden,encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_padding(src_sents, tgt_sents):\n",
    "    batch_size = len(src_sents)\n",
    "\n",
    "    max_src_len = max([len(sent) for sent in src_sents])\n",
    "    max_tgt_len = max([len(sent) for sent in tgt_sents])\n",
    "\n",
    "    padded_src_sents = np.zeros((batch_size, max_src_len))\n",
    "    padded_Yinput = np.zeros((batch_size, max_tgt_len))\n",
    "    padded_Ytarget = np.zeros((batch_size, max_tgt_len))\n",
    "\n",
    "    src_lens = []\n",
    "    tgt_lens = []\n",
    "\n",
    "    for i, sent in enumerate(zip(src_sents, tgt_sents)):\n",
    "        src_sent = sent[0]\n",
    "        y_input = sent[1][:-1]\n",
    "        y_target = sent[1][1:]\n",
    "\n",
    "        src_len = len(src_sent)\n",
    "        tgt_len = len(y_input)\n",
    "\n",
    "        padded_src_sents[i, :src_len] = src_sent\n",
    "        padded_Yinput[i, :tgt_len] = y_input\n",
    "        padded_Ytarget[i, :tgt_len] = y_target\n",
    "\n",
    "        src_lens.append(src_len)\n",
    "        tgt_lens.append(tgt_len)\n",
    "\n",
    "    return torch.LongTensor(padded_src_sents), src_lens, \\\n",
    "           torch.LongTensor(padded_Yinput), torch.LongTensor(padded_Ytarget), tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT(object):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, nvocab_src, nvocab_tgt, dropout_rate=0.2):\n",
    "        super(NMT, self).__init__()\n",
    "\n",
    "#         nvocab_src = len(vocab.src)\n",
    "#         nvocab_tgt = len(vocab.tgt)\n",
    "#         self.vocab = vocab\n",
    "        self.encoder = Encoder(nvocab_src, hidden_size, embed_size, input_dropout=dropout_rate, n_layers=1)\n",
    "        self.decoder = Decoder(nvocab_tgt, 2*hidden_size, embed_size,output_dropout=dropout_rate, n_layers=1)\n",
    "        LAS_params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optim.Adam(LAS_params, lr=0.001)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=0.5)\n",
    "        weight = torch.ones(nvocab_tgt)\n",
    "        self.loss = NLLLoss(weight=weight, mask=0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            # Move the network and the optimizer to the GPU\n",
    "            self.encoder = self.encoder.cuda()\n",
    "            self.decoder = self.decoder.cuda()\n",
    "            self.loss.cuda()\n",
    "\n",
    "\n",
    "    def __call__(self, src_sents, tgt_sents):\n",
    "        \"\"\"\n",
    "        take a mini-batch of source and target sentences, compute the log-likelihood of \n",
    "        target sentences.\n",
    "\n",
    "        Args:\n",
    "            src_sents: list of source sentence tokens\n",
    "            tgt_sents: list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        Returns:\n",
    "            scores: a variable/tensor of shape (batch_size, ) representing the \n",
    "                log-likelihood of generating the gold-standard target sentence for \n",
    "                each example in the input batch\n",
    "        \"\"\"\n",
    "        src_sents = self.vocab.src.words2indices(src_sents)\n",
    "        tgt_sents = self.vocab.tgt.words2indices(tgt_sents)\n",
    "        src_sents, src_len, y_input, y_tgt, tgt_len  = sent_padding(src_sents, tgt_sents)\n",
    "        src_encodings, decoder_init_state = self.encode(src_sents,src_len)\n",
    "        scores = self.decode(src_encodings, decoder_init_state, [y_input, y_tgt])\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def encode(self, src_sents, input_lengths):\n",
    "        \"\"\"\n",
    "        Use a GRU/LSTM to encode source sentences into hidden states\n",
    "\n",
    "        Args:\n",
    "            src_sents: list of source sentence tokens\n",
    "\n",
    "        Returns:\n",
    "            src_encodings: hidden states of tokens in source sentences, this could be a variable \n",
    "                with shape (batch_size, source_sentence_length, encoding_dim), or in orther formats\n",
    "            decoder_init_state: decoder GRU/LSTM's initial state, computed from source encodings\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src_sents,input_lengths)\n",
    "\n",
    "        return encoder_outputs, encoder_hidden\n",
    "\n",
    "\n",
    "    def decode(self, src_encodings, decoder_init_state, tgt_sents):\n",
    "        \"\"\"\n",
    "        Given source encodings, compute the log-likelihood of predicting the gold-standard target\n",
    "        sentence tokens\n",
    "\n",
    "        Args:\n",
    "            src_encodings: hidden states of tokens in source sentences\n",
    "            decoder_init_state: decoder GRU/LSTM's initial state\n",
    "            tgt_sents: list of gold-standard target sentences, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        Returns:\n",
    "            scores: could be a variable of shape (batch_size, ) representing the \n",
    "                log-likelihood of generating the gold-standard target sentence for \n",
    "                each example in the input batch\n",
    "        \"\"\"\n",
    "        tgt_input,tgt_target = tgt_sents\n",
    "        decoder_outputs, decoder_hidden = self.decoder(tgt_input, decoder_init_state, src_encodings)\n",
    "        loss = self.loss\n",
    "        loss.reset()\n",
    "        for step, step_output in enumerate(decoder_outputs):\n",
    "            batch_size = tgt_input.size(0)\n",
    "            loss.eval_batch(step_output.contiguous().view(batch_size, -1), tgt_target[:, step])\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        scores = loss.get_loss()\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def decode_without_bp(self, src_encodings, decoder_init_state, tgt_sents):\n",
    "        \"\"\"\n",
    "        Given source encodings, compute the log-likelihood of predicting the gold-standard target\n",
    "        sentence tokens\n",
    "\n",
    "        Args:\n",
    "            src_encodings: hidden states of tokens in source sentences\n",
    "            decoder_init_state: decoder GRU/LSTM's initial state\n",
    "            tgt_sents: list of gold-standard target sentences, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        Returns:\n",
    "            scores: could be a variable of shape (batch_size, ) representing the\n",
    "                log-likelihood of generating the gold-standard target sentence for\n",
    "                each example in the input batch\n",
    "        \"\"\"\n",
    "        tgt_input,tgt_target = tgt_sents\n",
    "        decoder_outputs, decoder_hidden = self.decoder(tgt_input, decoder_init_state, src_encodings)\n",
    "        loss = self.loss\n",
    "        loss.reset()\n",
    "        for step, step_output in enumerate(decoder_outputs):\n",
    "            batch_size = tgt_input.size(0)\n",
    "            loss.eval_batch(step_output.contiguous().view(batch_size, -1), tgt_target[:, step])\n",
    "\n",
    "        scores = loss.get_loss()\n",
    "\n",
    "        return decoder_outputs, scores\n",
    "\n",
    "    # TODO: sent_padding for only src\n",
    "    # def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
    "    def beam_search(self, src_sent, beam_size, max_decoding_time_step):\n",
    "        \"\"\"\n",
    "        Given a single source sentence, perform beam search\n",
    "\n",
    "        Args:\n",
    "            src_sent: a single tokenized source sentence\n",
    "            beam_size: beam size\n",
    "            max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n",
    "\n",
    "        Returns:\n",
    "            hypotheses: a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "\n",
    "        hypotheses = 0\n",
    "        return hypotheses\n",
    "    \n",
    "    # def evaluate_ppl(self, dev_data: List[Any], batch_size: int=32):\n",
    "    def evaluate_ppl(self, dev_data, batch_size):\n",
    "        \"\"\"\n",
    "        Evaluate perplexity on dev sentences\n",
    "\n",
    "        Args:\n",
    "            dev_data: a list of dev sentences\n",
    "            batch_size: batch size\n",
    "        \n",
    "        Returns:\n",
    "            ppl: the perplexity on dev sentences\n",
    "        \"\"\"\n",
    "\n",
    "        cum_loss = 0.\n",
    "        count = 0\n",
    "\n",
    "        # you may want to wrap the following code using a context manager provided\n",
    "        # by the NN library to signal the backend to not to keep gradient information\n",
    "        # e.g., `torch.no_grad()`\n",
    "\n",
    "        ref_corpus = []\n",
    "        hyp_corpus = []\n",
    "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "            ref_corpus.extend(tgt_sents)\n",
    "            src_sents = self.vocab.src.words2indices(src_sents)\n",
    "            tgt_sents = self.vocab.tgt.words2indices(tgt_sents)\n",
    "            src_sents, src_len, y_input, y_tgt, tgt_len = sent_padding(src_sents, tgt_sents)\n",
    "            src_encodings, decoder_init_state = self.encode(src_sents, src_len)\n",
    "            decoder_outputs, loss = self.decode_without_bp(src_encodings, decoder_init_state, [y_input, y_tgt])\n",
    "            cum_loss += loss\n",
    "            count += 1\n",
    "\n",
    "            # decoder outputs to word sequence\n",
    "            hyp_np = np.zeros((len(tgt_sents), len(decoder_outputs), len(self.vocab.tgt)))\n",
    "\n",
    "            for step in range(len(decoder_outputs)):\n",
    "                tmp = decoder_outputs[step].cpu().data.numpy()\n",
    "                # print(tmp.shape)\n",
    "                hyp_np[:, step, :] = tmp\n",
    "            # print(hyp_np.shape)\n",
    "\n",
    "            # converting softmax to word string\n",
    "            for b in range(hyp_np.shape[0]):\n",
    "                word_seq = []\n",
    "                for step in range(hyp_np.shape[1]):\n",
    "                    pred_idx = np.argmax(hyp_np[b,step,:])\n",
    "                    # print(pred_idx)\n",
    "                    if pred_idx == self.vocab.tgt.word2id['</s>']:\n",
    "                        break\n",
    "                    word_seq.append(self.vocab.tgt.id2word[pred_idx])\n",
    "                hyp_corpus.append(word_seq)\n",
    "\n",
    "            # tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting the leading `<s>`\n",
    "            # cum_tgt_words += tgt_word_num_to_predict\n",
    "\n",
    "        # ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "        for r, h in zip(ref_corpus, hyp_corpus):\n",
    "            print(r)\n",
    "            print(h)\n",
    "            print()\n",
    "        bleu = compute_corpus_level_bleu_score(ref_corpus, hyp_corpus)\n",
    "        print('bleu score: ', bleu)\n",
    "\n",
    "        return cum_loss / count\n",
    "\n",
    "    # @staticmethod\n",
    "    def load(self, model_path):\n",
    "\n",
    "        self.encoder.load_state_dict(torch.load(model_path + '-encoder'))\n",
    "        self.decoder.load_state_dict(torch.load(model_path + '-decoder'))\n",
    "\n",
    "    def save(self, model_save_path):\n",
    "        \"\"\"\n",
    "        Save current model to file\n",
    "        \"\"\"\n",
    "        torch.save(self.encoder.state_dict(), model_save_path + '-encoder')\n",
    "        torch.save(self.decoder.state_dict(), model_save_path + '-decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nmt/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7f43666b2e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-dc2c8c457b05>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size, hidden_size, nvocab_src, nvocab_tgt, dropout_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvocab_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mLAS_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAS_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvocab_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "nmt = NMT(5,10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nmt",
   "language": "python",
   "name": "nmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
