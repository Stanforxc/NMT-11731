{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import Attention\n",
    "from loss import Perplexity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = torch.LongTensor(5,10).random_(0,10) # batch * seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_var = torch.LongTensor(5,10).random_(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(10,10,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder.forward(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(10,10,2*10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nmt/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "result = decoder.forward(output_var,encoder_hidden,encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from generalRnn import BaseCoder\n",
    "from attention import Attention\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Decoder(BaseCoder):\n",
    "    def __init__(self, vocab_size, hidden_size, embedding_size, dropout=0.0, n_layers=1, bidirectional=False,attention=False,rnn=\"lstm\"):\n",
    "        super(Decoder,self).__init__(vocab_size, hidden_size,embedding_size,\n",
    "                dropout, n_layers, rnn)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = self.baseModel(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, \n",
    "                    batch_first=True,dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.output_size = vocab_size\n",
    "        self.attention_usage = attention\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.attention = Attention(self.hidden_size)\n",
    "\n",
    "        self.wsm = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_seq, encoder_hidden, encoder_outputs, func=F.log_softmax):\n",
    "        batch_size = input_seq.size(0)\n",
    "        max_length = input_seq.size(1)\n",
    "\n",
    "        # using cuda or not\n",
    "        inputs = input_seq\n",
    "        \n",
    "        # for bidrectional encoder\n",
    "        # encoder_hidden: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        decoder_hidden = tuple([torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2) for h in encoder_hidden])\n",
    "\n",
    "        outputs = []\n",
    "        # lengths = np.array([max_length] * batch_size)\n",
    "\n",
    "        prev = inputs[:, 0].unsqueeze(1)\n",
    "        for i in range(max_length):\n",
    "            softmax, decoder_hidden, attention = self.forward_helper(prev, decoder_hidden,encoder_outputs ,func)\n",
    "            output_seq = softmax.squeeze(1) # batch * seq_length\n",
    "            outputs.append(output_seq)\n",
    "            prev = output_seq.topk(1)[1] # max probability index\n",
    "\n",
    "        return outputs,decoder_hidden\n",
    "            \n",
    "\n",
    "\n",
    "    # could insert one parameter like: src_matrix\n",
    "    def forward_helper(self, decoder_input, decoder_hidden, encoder_outputs, func):\n",
    "        batch_size = decoder_input.size(0)\n",
    "        output_size = decoder_input.size(1)\n",
    "        embedded = self.embedding(decoder_input)\n",
    "        output,hidden = self.rnn(embedded, decoder_hidden)\n",
    "        output, attention = self.attention(output, encoder_outputs) # attention\n",
    "        softmax = func(self.wsm(output.view(-1, self.hidden_size)), dim=1).view(batch_size,output_size,-1)\n",
    "        return softmax, hidden, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from generalRnn import BaseCoder\n",
    "class Encoder(BaseCoder):\n",
    "    def __init__(self,vocab_size, hidden_size, embedding_size, dropout=0.0, n_layers=1, bidirectional=True,rnn=\"lstm\"):\n",
    "        super(Encoder, self).__init__(vocab_size, hidden_size,embedding_size,\n",
    "                dropout, n_layers, rnn)\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "\n",
    "        self.rnn = self.baseModel(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, \n",
    "                    batch_first=True,dropout=(0 if n_layers == 1 else dropout), bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_src = read_corpus(\"data/train.de-en.en.wmixerprep\", source='src')\n",
    "train_data_tgt = read_corpus(\"data/train.de-en.de.wmixerprep\", source='tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))\n",
    "vocab = pickle.load(open(\"VOCAB_FILE\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from docopt import docopt\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "\n",
    "from utils import read_corpus, batch_iter\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from torch import optim\n",
    "\n",
    "from loss import Perplexity\n",
    "from optim import Optimizer\n",
    "\n",
    "\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "class NMT(object):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        super(NMT, self).__init__()\n",
    "\n",
    "        nvocab_src = len(vocab.src)\n",
    "        nvocab_tgt = len(vocab.tgt)\n",
    "        self.encoder = Encoder(nvocab_src, hidden_size, embed_size, dropout=dropout_rate, n_layers=1)\n",
    "        self.decoder = Decoder(nvocab_tgt, 2*hidden_size, embed_size, n_layers=1)\n",
    "        LAS_params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optim.Adam(LAS_params, lr=0.01)\n",
    "        weight = torch.ones(nvocab_tgt)\n",
    "        self.loss = Perplexity(weight, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, src_sents, tgt_sents):\n",
    "        \"\"\"\n",
    "        take a mini-batch of source and target sentences, compute the log-likelihood of \n",
    "        target sentences.\n",
    "\n",
    "        Args:\n",
    "            src_sents: list of source sentence tokens\n",
    "            tgt_sents: list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        Returns:\n",
    "            scores: a variable/tensor of shape (batch_size, ) representing the \n",
    "                log-likelihood of generating the gold-standard target sentence for \n",
    "                each example in the input batch\n",
    "        \"\"\"\n",
    "        # src_encodings, decoder_init_state = self.encode(src_sents, tgt_sents)\n",
    "        src_sents = vocab.src.words2indices(src_sents)\n",
    "        tgt_sents = vocab.tgt.words2indices(tgt_sents)\n",
    "        src_sents, src_len, y_input, y_tgt, tgt_len  = sent_padding(src_sents, tgt_sents)\n",
    "        src_encodings, decoder_init_state = self.encode(src_sents,src_len)\n",
    "        scores = self.decode(src_encodings, decoder_init_state, [y_input, y_tgt])\n",
    "        print(scores)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def encode(self, src_sents,input_lengths):\n",
    "        \"\"\"\n",
    "        Use a GRU/LSTM to encode source sentences into hidden states\n",
    "\n",
    "        Args:\n",
    "            src_sents: list of source sentence tokens\n",
    "\n",
    "        Returns:\n",
    "            src_encodings: hidden states of tokens in source sentences, this could be a variable \n",
    "                with shape (batch_size, source_sentence_length, encoding_dim), or in orther formats\n",
    "            decoder_init_state: decoder GRU/LSTM's initial state, computed from source encodings\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src_sents,input_lengths)\n",
    "\n",
    "        return encoder_outputs, encoder_hidden\n",
    "\n",
    "    def decode(self, src_encodings, decoder_init_state, tgt_sents):\n",
    "        \"\"\"\n",
    "        Given source encodings, compute the log-likelihood of predicting the gold-standard target\n",
    "        sentence tokens\n",
    "\n",
    "        Args:\n",
    "            src_encodings: hidden states of tokens in source sentences\n",
    "            decoder_init_state: decoder GRU/LSTM's initial state\n",
    "            tgt_sents: list of gold-standard target sentences, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        Returns:\n",
    "            scores: could be a variable of shape (batch_size, ) representing the \n",
    "                log-likelihood of generating the gold-standard target sentence for \n",
    "                each example in the input batch\n",
    "        \"\"\"\n",
    "        tgt_input,tgt_target = tgt_sents\n",
    "        decoder_outputs, decoder_hidden = self.decoder(tgt_input, decoder_init_state, src_encodings)\n",
    "        loss = self.loss\n",
    "        loss.reset()\n",
    "        for step, step_output in enumerate(decoder_outputs):\n",
    "            batch_size = tgt_input.size(0)\n",
    "            loss.eval_batch(step_output.contiguous().view(batch_size, -1), tgt_target[:, step])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        scores = loss.get_loss()\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
    "        \"\"\"\n",
    "        Given a single source sentence, perform beam search\n",
    "\n",
    "        Args:\n",
    "            src_sent: a single tokenized source sentence\n",
    "            beam_size: beam size\n",
    "            max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n",
    "\n",
    "        Returns:\n",
    "            hypotheses: a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "\n",
    "        return hypotheses\n",
    "\n",
    "#     def evaluate_ppl(self, dev_data: List[Any], batch_size: int=32):\n",
    "#         \"\"\"\n",
    "#         Evaluate perplexity on dev sentences\n",
    "\n",
    "#         Args:\n",
    "#             dev_data: a list of dev sentences\n",
    "#             batch_size: batch size\n",
    "        \n",
    "#         Returns:\n",
    "#             ppl: the perplexity on dev sentences\n",
    "#         \"\"\"\n",
    "\n",
    "#         cum_loss = 0.\n",
    "#         cum_tgt_words = 0.\n",
    "\n",
    "#         # you may want to wrap the following code using a context manager provided\n",
    "#         # by the NN library to signal the backend to not to keep gradient information\n",
    "#         # e.g., `torch.no_grad()`\n",
    "\n",
    "#         for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "#             loss = -model(src_sents, tgt_sents).sum()\n",
    "\n",
    "#             cum_loss += loss\n",
    "#             tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting the leading `<s>`\n",
    "#             cum_tgt_words += tgt_word_num_to_predict\n",
    "\n",
    "#         ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "\n",
    "#         return ppl\n",
    "\n",
    "#     @staticmethod\n",
    "#     def load(model_path: str):\n",
    "#         \"\"\"\n",
    "#         Load a pre-trained model\n",
    "\n",
    "#         Returns:\n",
    "#             model: the loaded model\n",
    "#         \"\"\"\n",
    "\n",
    "#         return model\n",
    "\n",
    "#     def save(self, path: str):\n",
    "#         \"\"\"\n",
    "#         Save current model to file\n",
    "#         \"\"\"\n",
    "\n",
    "#         raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/nmt/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = NMT(128,256,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_sents, tgt_sents in batch_iter(train_data, batch_size=100, shuffle=True):\n",
    "#     src_length = [len(sent) for sent in src_sents]\n",
    "#     tgt_length = [len(sent) for sent in tgt_sents]\n",
    "#     model(src_sents, tgt_sents)\n",
    "    src_sents = vocab.src.words2indices(src_sents)\n",
    "    tgt_sents = vocab.tgt.words2indices(tgt_sents)\n",
    "    sent_padding(src_sents, tgt_sents)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_padding(src_sents, tgt_sents):\n",
    "    batch_size = len(src_sents)\n",
    "    assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "    max_src_len = len(src_sents[0])\n",
    "    max_tgt_len = len(tgt_sents[0])\n",
    "\n",
    "    padded_src_sents = np.zeros((batch_size, max_src_len))\n",
    "    padded_Yinput = np.zeros((batch_size, max_tgt_len))\n",
    "    padded_Ytarget = np.zeros((batch_size, max_tgt_len))\n",
    "\n",
    "    src_lens = []\n",
    "    tgt_lens = []\n",
    "\n",
    "    for i, sent in enumerate(zip(src_sents, tgt_sents)):\n",
    "        src_sent = sent[0]\n",
    "        y_input = sent[1][:-1]\n",
    "        y_target = sent[1][1:]\n",
    "\n",
    "        src_len = len(src_sent)\n",
    "        tgt_len = len(y_input)\n",
    "\n",
    "        padded_src_sents[i, :src_len] = src_sent\n",
    "        padded_Yinput[i, :tgt_len] = y_input\n",
    "        padded_Ytarget[i, :tgt_len] = y_target\n",
    "\n",
    "        src_lens.append(src_len)\n",
    "        tgt_lens.append(tgt_len)\n",
    "\n",
    "    return torch.LongTensor(padded_src_sents), src_lens, \\\n",
    "           torch.LongTensor(padded_Yinput), torch.LongTensor(padded_Ytarget), tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nmt",
   "language": "python",
   "name": "nmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
